%!TEX root = ../thesis_polimi.tex
\chapter{Experimental Validation} % (fold)
\label{chap:validation}
\start{I}{n} this chapter we describe the results that validated the effectiveness of \thesystem.
We had two goals in mind: first we wanted to confirm the effectiveness of the
\important{Classifier}, which is easily quantifiable by terms of the
confusion matrix produced; second, we wanted to test \thesystem's goodness
when deployed in the wild.
To this end we let the system
analyze one week of real DNS passive data and see if it \emph{classifies} unseen domains belonging to known threats and if it \emph{detects} new threats.
This latter test is far from trivial: Given the
massive quantitative of data it is very hard even to give a rough estimation
of false negatives, as it would require to manually check every domain discarded
in the \important{Filtering Phase}. Moreover some results, though important,
cannot be quantified: For instance \thesystem was able to correctly separate
clusters of domains belonging to the same threat, but using different DGAs, and
to merge together two clusters of domains employed by \texttt{Palevo}, brilliantly understanding that they belonged to the same botnet.
Both of the
aforementioned test are presented in the following of this chapter, highlighting
the goodness of the obtained results.
\paragraph{Chapter Organization} The remainder of the chapter is organized in
the following fashion:
\begin{itemize}
\item in Section~\ref{sec:goals} we precisely set our goals, what we want to
prove with our experiments;
\item in Section~\ref{sec:dataset} we describe the dataset employed in our
experiments;
\item in Section~\ref{sec:the_classifier} we test the effectiveness of the
classifier;
\item in Section~\ref{sec:cerberus_in_the_wild} we run a one week simulation
to see how the system would behave once deployed.
\end{itemize}
\newpage
\section{Goals} % (fold)
\label{sec:goals}
\sectionstart{A}{s} discussed in Section~\ref{sub:phoenix_detecting_dga_based_botnets},
\phoenix suffers from two main kind of shortcomings, one \emph{conceptual} whereas the other
concerns the validation. The conceptual shortcomings regard the use of
\emph{ad hoc} parameters when it comes to classifying unseen domains, an approach
that is prone to overfitting and, in our case, it is not able to correctly
classify AGDs that feature a different TLD or a domain length that evades the
previous thresholds. With our first experiment we want to validate our classifier: This test is described in Section~\ref{sec:the_classifier}.
The second kind of shortcoming has to do with validation. \phoenix was not tested in the
wild, whereas this is a mandatory step for a detection system that wants
to be deployed in the real world.
In \thesystem we have addressed this issue, and in Section~\ref{sec:cerberus_in_the_wild} we confirm this claim.
% section goals (end)
\section{Dataset} % (fold)
\label{sec:dataset}
\sectionstart{C}{erberus}' tests employed real passive DNS data collected in the
wild. With the term \emph{passive}, we refer to the technique invented
by~\citet{weimer2005passive} in 2004, called ``Passive DNS Replication'', to obtain
Domain Name System data from production networks, and store it in a database
for later reference~\cite{weimer2005passive}.
The sensors, deployed as close as possible to large caching servers, collect the
data and send it to an \emph{analyzer} module, where the data is filtered.
After the filtering process, the data is transformed in a format that makes
the querying process easier, and stored in a database.
We were able to get access to approximately three months of data, from the 12th
of January to the 15th of March 2013. About 609 M of DNS queries/replies were
collected by the ISC/SIE monitor. Data statistics are summarized in Table~\ref{tab:stats}.
% We decided
% also to investigate the presence of punycode records, which resulted to be
% approximately the 0.007\% of the dataset. We think that attackers that would
% start employing punycode domains would get easily spotted, as there would be
% peaks of NXDOMAIN DNS answers for punycode domains, a phenomenon that could
% immediately identified by defenders because of its peculiarity.
\begin{table}[h!tp]
\centering
\begin{tabular}{rl}
Begin of Recording & Sat, 12 Jan 2013 18:19:57 GMT \\
End of Recording   & Fri, 15 Mar 2013 23:37:11 GMT \\
Total Records      & 608,958,044 \\
% Punycode Records   & 41,474 \\
\end{tabular}
\caption{ISC/SIE data summary statistics.}
\label{tab:stats}
\end{table}
The data was divided into snapshots of twenty minutes on average, counting
200,000 DNS messages, of which about 50,000 were successful (i.e., non \texttt{NXDOMAIN})
DNS replies.
\section{The Classifier} % (fold)
\label{sec:the_classifier}
\sectionstart{W}{e} tested \thesystem's classifier against the ground truth
generated by \phoenix, i.e., the clusters generated during the \important{Bootstrap Phase}. We employed data \emph{automatically} labeled by \thesystem and
not data \emph{manually} labeled by humans as we wanted to run a test as close
as possible to the real case scenario. \thesystem in fact, will automatically
produce the labeled records later to be used for classification, and we wanted
to have our experiment set up to reproduce such situation. Note that, though
automatically labeled, the clusters' maliciousness and quality were manually
assessed by~\citet{schiavoni2013} and therefore represent a valid dataset to
run our experiment.
\subsection{Accuracy} % (fold)
\label{sub:accuracy}
We considered four clusters that counted 1,100 samples, thus we could
measure the accuracy depending on the number of domains used up to 1,000 samples, and
leave the remaining 100 domains for testing. We validated the classifier using
\emph{repeated random sub-sampling validation}, ten times for each amount of points.
This means that, for instance, for ten times we randomly selected 200 points to train
the classifier and 100 points to test it: This validation method allows to reveal the effects of overfitting, if any.  We collected the overall accuracies from
the confusion matrix along with the computation time. We repeated this operation
until we counted 1,000 points in the training set, step 100. This data is reported in
Table~\ref{tab:classifier_stats}.
\begin{table}[!htp]
\centering
\pgfplotstabletypeset[%
columns={points,avg,std,min,max,time},
columns/points/.style={
column name=\textsc{Domains},
column type = {r}
},
columns/max/.style={
column name=\textsc{Max},
precision=3,
dec sep align,
fixed, fixed zerofill
},
columns/min/.style={
column name=\textsc{Min},
precision=3,
dec sep align,
fixed, fixed zerofill
},
columns/std/.style={
column name=\textsc{Std}
},
columns/avg/.style={
column name=\textsc{Avg},
precision=3,
dec sep align,
fixed, fixed zerofill,
},
columns/time/.style={
column name=\textsc{Time} (s),
dec sep align
},
every head row/.style={
before row={%
\toprule
& \multicolumn{8}{c}{\textsc{Accuracy}} &  \\
\cmidrule{2-8}
},
after row=\midrule},
every last row/.style={
after row=\bottomrule},
]{data/classifier.dat}
\caption{Cerberus classifier accuracy statistics.}
\label{tab:classifier_stats}
\end{table}
Overall accuracy grows and then stabilizes at about 93\% from 800 points on
(see Figure~\ref{fig:cerberus_accuracies}). For this reason the implementation
of the \important{Classifier} has a sampling upper bound of 800 points to be randomly selected
from the clusters to train the SVM used for classification.
\begin{figure}[!htp]
\centering
\begin{tikzpicture}
\begin{axis}[%
xlabel=Accuracy,
ylabel=Points,
width=.9\linewidth,
axis x line*=bottom,
axis y line*=left,
major tick style={draw=none},
y=.5cm,
ymax=10,
ytick={1,2,3,4,5,6,7,8,9},
xtick={0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95},
xticklabels={0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95},
yticklabels={200, 300, 400, 500, 600, 700, 800, 900, 1000},
% boxplot/draw direction=y,
boxplot/every box/.style={fill=Tufte, draw=Tufte},
boxplot/every median/.style={draw, ultra thick},] %
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.915\\  0.9\\  0.885\\  0.92\\  0.88\\  0.895\\  0.9\\  0.8975\\  0.8775\\  0.8825\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.93\\  0.88\\  0.905\\  0.925\\  0.9325\\  0.9175\\  0.91\\  0.92\\  0.8875\\  0.8925\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.9175\\  0.9125\\  0.915\\  0.9275\\  0.9375\\  0.8875\\  0.935\\  0.9025\\  0.9275\\  0.9125\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.92\\  0.92\\  0.9175\\  0.92\\  0.915\\  0.8975\\  0.9125\\  0.9025\\  0.9325\\  0.93\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.9275\\  0.905\\  0.92\\  0.9275\\  0.94\\  0.915\\  0.92\\  0.9275\\  0.925\\  0.895\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.915\\  0.8975\\  0.9325\\  0.9075\\  0.9075\\  0.94\\  0.9075\\  0.915\\  0.93\\  0.9125\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.9225\\  0.9325\\  0.91\\  0.9375\\  0.92\\  0.9475\\  0.9325\\  0.9075\\  0.9475\\  0.9225\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.8925\\  0.9325\\  0.93\\  0.9325\\  0.9475\\  0.94\\  0.92\\  0.92\\  0.92\\  0.9225\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
0.95\\  0.925\\  0.905\\  0.935\\  0.9275\\  0.935\\  0.9375\\  0.91\\  0.925\\  0.9225\\
};
\end{axis}
\end{tikzpicture}
\caption{Cerberus classifier accuracies.}
\label{fig:cerberus_accuracies}
\end{figure}
% subsection accuracy (end)
\subsection{Analysis of Classification Errors} % (fold)
\label{sub:analysis_of_classification_errors}
We report in Figure~\ref{fig:confusion} one of the confusion matrices, obtained using
1,000 samples for training, during the validation. As you can see in the picture the best performances
are obtained with cluster \texttt{a}, while the worst with cluster \texttt{d}.
This holds for all the sub-sampling validations.
This is due to the different lengths and distribution of characters that
characterize the clusters.
Cluster \texttt{a} in fact, is composed of domains that,
on average, count more than thirty characters and are produced using a DGA
that leverages a common hashing function over the date. This kind
of algorithm generates domains that share a sort of ``pattern'' which
allows the classifier to better recognize such domains. On the other hand,
cluster \texttt{d}'s domains are three characters long: Therefore it is harder
to find ``patterns'' shared by the domains and consequently harder for the
\textbf{Classifier} to classify the domains correctly. This is confirmed by looking at
the distribution of the distances among the domains in the clusters,
computed using the Subsequence String Kernel and reported in Figure~\ref{fig:clusters_distribution}. It is evident that distances
in cluster \texttt{a} have a normal distribution with a very low mean and
variance, which indicate very high intra-cluster similarity, whereas cluster \texttt{d}'s distances are randomly distributed and
exhibit higher values, which indicate low intra-cluster similarity. Note that despite of this
weakness, the overall accuracy allows us to use this classifier when
\thesystem is deployed in the wild.
\begin{figure}[!htp]
\sffamily
\begin{minipage}{.5\textwidth}
\centering
\begin{tabular}{crcccc}
& & \multicolumn{4}{c}{Predicted} \\
& & a     & b     & c     & d  \\
\cmidrule(r){2-6}
\multirow{4}{*}{\rotatebox{90}{Actual}} & a & \bverb+100+ & \verb+ 0+ & \verb+ 0+ & \verb+ 0+  \\
& b   & \verb+  1+     &  \bverb+92+    &  \verb+ 6+    & \verb+ 1+  \\
& c   & \verb+  2+     &  \verb+ 0+    &  \bverb+98+    & \verb+ 0+  \\
& d   & \verb+  3+     &  \verb+ 0+    &  \verb+ 6+    & \bverb+91+ \\
\end{tabular}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
\begin{tabular}{l}
a \\
\midrule
\verb+caaa89e...d4ca925b3e2.co.cc+ \\
\verb+f1e01ac...51b64079d86.co.cc+ \\
b \\
\midrule
\verb+kdnvfyc.biz+ \\
\verb+wapzzwvpwq.info+ \\
c \\
\midrule
\verb+jhhfghf7.tk+ \\
\verb+faukiijjj25.tk+ \\
d \\
\midrule
\verb+cvq.com+ \\
\verb+epu.org+ \\
\end{tabular}
\end{minipage}
\caption{Cerberus SSK classifier confusion matrix.}
\label{fig:confusion}
\end{figure}
\begin{figure}[!htp]
\centering
\begin{tikzpicture}
\begin{groupplot}[
group style={
group name=histograms,
group size=1 by 2,
xlabels at=edge bottom,
xticklabels at=edge bottom,
vertical sep=0em
},
ybar,
scaled y ticks = false,
width=\textwidth,
height=5cm,
axis x line*=bottom,
axis y line*=left,
major tick style={draw=none},
ymin=0,
xmax=0.9,
xmin=0.1,
ytick={0, 5000, 10000, 15000},
yticklabels={0, {5,000}, {10,000}, {15,000}},
xtick={0.2, 0.4, 0.6, 0.6, 0.8},
xticklabels={0.2, 0.4, 0.6, 0.6, 0.8},
]
\nextgroupplot
\addplot[%
fill=Tufte,
draw=none
] coordinates
{(0.162101122991, 666)
(0.179813448003, 5472)
(0.197525773016, 12356)
(0.215238098028, 15188)
(0.23295042304, 13652)
(0.250662748052, 9837)
(0.268375073065, 6418)
(0.286087398077, 3904)
(0.303799723089, 2291)
(0.321512048101, 1490)
(0.339224373114, 963)
(0.356936698126, 735)
(0.374649023138, 661)
(0.39236134815, 586)
(0.410073673163, 584)
(0.427785998175, 632)
(0.445498323187, 582)
(0.463210648199, 611)
(0.480922973211, 520)
(0.498635298224, 513)
(0.516347623236, 527)
(0.534059948248, 435)
(0.55177227326, 373)
(0.569484598273, 284)
(0.587196923285, 235)
(0.604909248297, 160)
(0.622621573309, 71)
(0.640333898322, 35)
(0.658046223334, 15)
(0.675758548346, 4)} ;
\nextgroupplot
\addplot[fill=Tufte,draw=none] coordinates
{(0.159564628861, 17)
(0.183793974484, 308)
(0.208023320108, 355)
(0.232252665732, 1698)
(0.256482011356, 2443)
(0.280711356979, 1379)
(0.304940702603, 4656)
(0.329170048227, 5166)
(0.353399393851, 2948)
(0.377628739474, 3223)
(0.401858085098, 4565)
(0.426087430722, 2949)
(0.450316776346, 3744)
(0.474546121969, 1948)
(0.498775467593, 1739)
(0.523004813217, 2326)
(0.54723415884, 5792)
(0.571463504464, 4148)
(0.595692850088, 4207)
(0.619922195712, 2952)
(0.644151541335, 1973)
(0.668380886959, 864)
(0.692610232583, 601)
(0.716839578207, 664)
(0.74106892383, 2752)
(0.765298269454, 4917)
(0.789527615078, 6270)
(0.813756960702, 4136)
(0.837986306325, 967)
(0.862215651949, 93)} ;
\end{groupplot}
\end{tikzpicture}
\caption{From the top: cluster \texttt{a}'s and cluster \texttt{d}'s distances distributions.}
\label{fig:clusters_distribution}
\end{figure}
% subsection analysis_of_classification_errors (end)
\subsection{Training Speed} % (fold)
\label{sub:training_speed}
Training time grows, in a linear fashion, from 14.13 to 101.49 seconds
(see Figure~\ref{fig:cerberus_training_time}). As \textsc{Cerberus} is
designed to analyze a \emph{live} stream of DNS data, the training time is
not negligible, as it could make the classification process too long.
To address this issue we can store the trained SVM machines: For instance if
an unseen domain $d$ can belong either to cluster $\alpha$ or cluster $\beta$,
\thesystem trains the SVM using cluster $\alpha$ and cluster $\beta$. Then this
machine is stored as a binary file using the \texttt{cPickle} \texttt{Python}
library: When another unseen domain $g$ can belong either to cluster $\alpha$ or
cluster $\beta$, \thesystem retrieves the stored trained SVM, loads it in
memory and uses it to label $g$, thus drastically improving the performances.
\begin{figure}[!htp]
\centering
\begin{tikzpicture}[font=\sffamily\sansmath]
\begin{axis}[
width=.9\linewidth,
xlabel=Training Time (s),
ylabel=Points,
major tick style={draw=none},
axis x line*=bottom,
axis y line*=left,
y=.5cm,
ytick={1,2,3,4,5,6,7,8,9},
xtick={10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110},
xticklabels={10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110},
yticklabels={200, 300, 400, 500, 600, 700, 800, 900, 1000},
boxplot/every box/.style={fill=Tufte, draw=Tufte},
boxplot/every median/.style={draw, ultra thick},
]
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
14.450056076\\ 14.3151428699\\ 15.0587539673\\ 14.6330120564\\ 13.1712338924\\
13.5638680458\\ 13.8222289085\\ 14.326633215\\ 14.268102169\\ 13.6863639355\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
24.2474851608\\ 21.8814628124\\ 20.7104799747\\ 23.4123110771\\ 21.5171511173\\
21.6167340279\\ 20.5072031021\\ 21.8328018188\\ 18.9486649036\\ 18.9871139526\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
31.860227108\\ 28.9508159161\\ 31.77709198\\ 31.860227108\\ 29.5555529594\\
28.0266561508\\ 26.9642550945\\ 28.0694839954\\ 26.931540966\\ 28.8569948673\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
42.6774580479\\ 42.7213740349\\ 42.1484789848\\ 45.2905337811\\ 42.620041132\\
46.0541510582\\ 42.7060930729\\ 42.0425620079\\ 39.8944818974\\ 38.2909250259\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
55.6774950027\\ 55.447537899\\ 51.9851491451\\ 47.7803220749\\ 47.7598872185\\
47.7725110054\\ 52.6958119869\\ 57.3658189774\\ 47.0200750828\\ 45.3178730011\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
68.4127650261\\ 64.5384910107\\ 70.2506511211\\ 58.2789590359\\ 60.0601868629\\
61.8797900677\\ 62.2854321003\\ 64.2296421528\\ 58.6308290958\\ 58.5854849815\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
76.3239531517\\ 74.2242810726\\ 70.2646820545\\ 71.9479129314\\ 69.1475520134\\
78.8463850021\\ 78.1692609787\\ 72.8085451126\\ 71.4164860249\\ 72.7233030796\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
87.4063670635\\ 86.3149549961\\ 80.9244608879\\ 81.8638818264\\ 88.5054130554\\
87.955684185\\ 92.3891918659\\ 89.4988970757\\ 84.9471900463\\ 85.7984118462\\
};
\addplot[boxplot] table[row sep=\\, y index=0] {
data\\
102.08713603\\ 105.802442074\\ 105.268303156\\ 103.374116182\\ 93.2224390507\\
102.118638992\\ 109.901737928\\ 102.901645899\\ 97.8026940823\\ 92.4691579342\\
};
\end{axis}
\end{tikzpicture}
\caption{Cerberus classifier training time.}
\label{fig:cerberus_training_time}
\end{figure}
% subsection training_speed (end)
% section the_classifier (end)
\section{Cerberus in the Wild} % (fold)
\label{sec:cerberus_in_the_wild}
\sectionstart{T}{his} is the main test to prove the effectiveness of \thesystem.
We decided to run a batch experiment of
one week and one day of deployment in the wild, using real data from the aforementioned dataset (see Section~\ref{sec:dataset}). During the first week \thesystem
leveraged the ground truth generated in the \important{Bootstrap Phase} to classify those unseen domains that would share their IP address with the clusters
in the ground truth. Those unseen domains that would not share the IP address with any of the clusters were considered ``suspicious'', as were not discarded by the \important{Filtering Phase} (i.e., they are likely-malicious), and stored in a database. Then, after one week time passed, the \textbf{Time Detective} ran the
clustering routine on these ``suspicious'' domains: New clusters were found and added to the ground truth. The day
after \thesystem used that knowledge to
successfully label unseen domains belonging to previously unknown threats, drastically
augmenting the number of detected malicious domains.
\subsection{The Bootstrap} % (fold)
\label{sub:the_bootstrap_validation}
Before starting classifying data, \thesystem can be bootstrapped: If this happens the system can leverage the knowledge obtained to classify unseen domains. Otherwise \thesystem starts to function with no knowledge and will build its ground truth throughout time in an automatic fashion. We decided to bootstrap the system, in order to see how \thesystem behaves \emph{before} and \emph{after}
it increases its knowledge, and
to use the blacklist provided by \textsc{Exposure}~\cite{bilge2011exposure},
thus providing \thesystem with the same knowledge used to feed Phoenix~\cite{schiavoni2013}. Once \textbf{The Bootstrap} is completed, \thesystem
possesses a list of eleven clusters of malicious domains likely generated by the
same DGA: Among others we find clusters that \citet{schiavoni2013} confirmed  referring to \texttt{Palevo} and \texttt{Conficker}.
Two of the eleven clusters generated by \phoenix in
\important{Bootstrap Phase} are reported in Table~\ref{tab:phoenix_clusters}.
\begin{table}[!htp]
\begin{minipage}{.5\textwidth}
\centering
\begin{tabular}{rp{2.8cm}}
\multicolumn{2}{l}{\textsc{Cluster f105c}} \\
\midrule
Threat: & Palevo \\
IPs: & \texttt{176.74.176.175} \newline \texttt{208.87.35.107} \newline \newline \\
Domains: & \texttt{cvq.com} \newline \texttt{epu.org} \newline \texttt{bwn.org} \\
\end{tabular}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
\begin{tabular}{rp{2.8cm}}
\multicolumn{2}{l}{\textsc{Cluster 0f468}} \\
\midrule
Threat: & Sality \\
IPs: & \texttt{217.119.57.22} \newline \texttt{91.215.158.57} \newline \texttt{178.162.164.24} \newline \texttt{94.103.151.195} \\
Domains: & \texttt{jhhfghf7.tk} \newline \texttt{faukiijjj25.tk} \newline \texttt{pvgvy.tk} \\
\end{tabular}
\end{minipage}
\caption{Two of the eleven clusters produced by Phoenix.}
\label{tab:phoenix_clusters}
\end{table}
% subsection the_bootstrap (end)
\subsection{Collecting Data} % (fold)
\label{sub:collecting_data}
The test started the day February, 7th 2013. At the end of the week, February, 14th 2013,
roughly 22,000,000 DNS replies were analyzed, 187 domains were classified as malicious and
labeled using the ground truth provided by \phoenix in the \textbf{Bootstrap Phase} (see Table~\ref{tab:classified}). We searched VirusTotal for the labeled domains
and we were able to prove that
167 domains belonged to the \texttt{Conficker} botnet, while the remainder
to other botnets or generic malware, including the \texttt{Flashback} botnet.
Moreover 3,576 domains considered by \thesystem ``suspicious'' (i.e., they were
not filtered out by the \important{Filtering Phase}) were stored together
with their IP addresses: We counted exactly 1,300 distinct IP addresses, which
means that multiple ``suspicious'' domains resolved to the same IP.
\begin{table}[!htp]
\centering
\begin{tabular}{rp{2.8cm}p{2.8cm}}
\multicolumn{2}{l}{\textsc{Labeled 07e21}} & \\
\midrule
Threat: & Conficker  & \\
Domains: & \texttt{hhdboqazof.biz} \newline \texttt{poxqmrfj.biz} \newline \texttt{hcsddszzzc.ws} & \texttt{tnoucgrje.biz} \newline \texttt{gwizoxej.biz} \newline  \texttt{jnmuoiki.biz} \\
\end{tabular}
\caption{A sample of the malicious domains classified during the first week.}
\label{tab:classified}
\end{table}
% subsection collecting_data (end)
\subsection{Producing New Knowledge} % (fold)
\label{sub:producing_new_knowledge}
At the end of the week \thesystem performed the clustering routine over
the domains resolving to suspicious IP addresses.  As described in
Section~\ref{par:dbscan_clustering}, the clustering routine is performed only
after grouping the IP addresses by Autonomous System. We report in
Table~\ref{tab:as} the main ASs involved in the analysis.
\begin{table}[!htp]
\centering
\begin{tabular}{lp{7cm}c}
\toprule
\textsc{AS} & \textsc{Network Name} & \textsc{Country} \\
\midrule
15456  & INTERNETX-AS InterNetX GmbH  & DE \\
22489  & Castle Access Inc            & UK \\
47846  & Sedo GmbH                    & DE \\
53665  & BODIS-1 - Bodis, LLC         & CN \\
\bottomrule
\end{tabular}
\caption{Autonomous Systems of the domains in clustering phase.}
\label{tab:as}
\end{table}
We applied the clustering routine, which yielded 47 clusters, of which the
bigger ones (counting more than 25 elements) are reported in Table~\ref{tab:clusters} along with the related threat.
\begin{table}[!htp]
\centering
\begin{tabular}{lcp{3cm}r}
\toprule
\textsc{Threat} & \textsc{AS} & \textsc{IPs} & \textsc{Size} \\
\midrule
Sality & 15456  & \texttt{62.116.181.25} & 26 \\
Palevo & 53665  & \texttt{199.59.243.118} & 40 \\
Jadtre* & 22489  & \texttt{69.43.161.180} \newline \texttt{69.43.161.174} & 173 \\
Jadtre** & 22489  & \texttt{69.43.161.180} & 37 \\
Jadtre*** & 22489  & \texttt{69.43.161.167} & 47 \\
Hiloti & 22489  & \texttt{69.43.161.167} & 24 \\
Palevo & 47846  & \texttt{82.98.86.171} \newline \texttt{82.98.86.176} \newline \texttt{82.98.86.175}
\newline \texttt{82.98.86.167} \newline \texttt{82.98.86.168} \newline \texttt{82.98.86.165}
& 142 \\
Jusabli & 30069 & \texttt{69.58.188.49} & 73 \\
Generic Trojan & 12306 & \texttt{82.98.86.169} \newline \texttt{82.98.86.162} \newline \texttt{82.98.86.178}
\newline \texttt{82.98.86.163} &  57 \\
\bottomrule
\end{tabular}
\caption{Cerberus' new clusters (asterisks to match Table~\ref{tab:jadtre}).}
\label{tab:clusters}
\end{table}
This means that, taking as input only the
passive DNS traffic, \thesystem was able to identify, in a completely
automatic fashion, groups of IPs that are associated to malicious botnet
activity (e.g., C\&C). This is new knowledge, which an investigator can use to
find new botnet servers.
There are three clusters, two of which
sharing the IP address \texttt{69.43.161.180} and all of three residing in the same
AS, that are labeled with the same threat, \texttt{Jadtre}\footnote{\url{http://www.microsoft.com/security/portal/threat/encyclopedia/entry.aspx?Name=TrojanDownloader:Win32/Jadtre.A}}. The reason why they are not one cluster,
though belonging to the same threat, is because they were generated using three
different DGAs, as it is clear from Table~\ref{tab:jadtre}. This proves that
\thesystem is able of a fine grained detection of malicious activities, showing
its capability of isolating not only different threats, but different DGA used
by the same threat.
\begin{table}
\centering
\begin{tabular}{lp{2.8cm}p{3.3cm}p{3.3cm}}
\toprule
\textsc{Cluster} & \textsc{IP} & \multicolumn{2}{l}{\textsc{Sample Domains}} \\
\midrule
Jadtre*   & \texttt{69.43.161.180} \newline \texttt{69.43.161.174} &
\texttt{379.ns4000wip.com} \newline \texttt{418.ns4000wip.com} \newline \texttt{285.ns4000wip.com} &
\texttt{78.ns4000wip.com} \newline \texttt{272.ns4000wip.com} \newline \texttt{98.ns4000wip.com}\\
Jadtre**  & \texttt{69.43.161.180} & \texttt{391.wap517.net} \newline \texttt{251.wap517.net} \newline
\texttt{340.wap517.net} & \texttt{137.wap517.net} \newline \texttt{203.wap517.net} \newline \texttt{128.wap517.net}
\\
Jadtre*** & \texttt{69.43.161.167} &
\texttt{388.ns768.com} \newline \texttt{353.ns768.com} \newline \texttt{296.ns768.com} & \texttt{312.ns768.com} \newline \texttt{153.ns768.com} \newline \texttt{30.ns768.com} \\
\bottomrule
\end{tabular}
\caption{Jadtre threats sample domains.}
\label{tab:jadtre}
\end{table}
The new clusters were then added to the ground truth. \thesystem ran a
\emph{similarity check} to see whether the new clusters should be
merged together with the old ones. The Welch's test told that
there was not enough statistical evidence to consider
clusters \emph{a} and \emph{b}, reported in Table~\ref{tab:merging},
dissimilar, thus \thesystem decided to merge
them together. Further investigations\footnote{\url{https://palevotracker.abuse.ch/}} confirmed that the IP addresses from both cluster \emph{a} and cluster \emph{b} belonged to \texttt{Palevo} C\&Cs. This means that \thesystem was able
to understand that the domains of the two clusters, the first generated by \phoenix from the
\textsc{Exposure} blacklist, and the second discovered by \thesystem analyzing
real passive DNS data, were produced by the same DGA. This discovery lead the
ground truth to be enriched, as the IP address \texttt{199.59.243.118} was
added to the cluster, together with the domains. This means that \thesystem was
able to successfully discover a \emph{migration} by leveraging only the
linguistic features computed by the SSK.
\begin{table}[h!tp]
\begin{minipage}{.5\textwidth}
\centering
\begin{tabular}{lp{2.5cm}}
\textsc{Cluster a} & \\
\midrule
IPs:           & \verb+176.74.176.175+ \newline \verb+208.87.35.107+ \newline \newline \newline \newline \\
Sample Domains & \texttt{cvq.com} \newline \texttt{epu.org} \newline \texttt{bwn.org} \newline \texttt{lxx.net} \\
\end{tabular}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
\begin{tabular}{lp{2.5cm}}
\textsc{Cluster b} & \\
\midrule
IPs: & \verb+82.98.86.171+ \newline \verb+82.98.86.176+ \newline \verb+82.98.86.175+
\newline \verb+82.98.86.167+ \newline \verb+82.98.86.168+ \newline
\verb+82.98.86.165+ \\
Sample Domains & \texttt{knw.info} \newline \texttt{rrg.info} \newline \texttt{nhy.org} \newline \texttt{ydt.info} \\
\end{tabular}
\end{minipage}
\caption{Clusters merged by \thesystem.}
\label{tab:merging}
\end{table}
After the new clusters were added to the ground truth, \thesystem started
again the \textbf{Detection Phase}, leveraging the increased knowledge: The next day \thesystem
classified 319 malicious domains, while during the whole previous week counted
187 malicious domains, on average 26 domains a day. Hence, \thesystem was able to increase its knowledge in
a completely automatic and unsupervised fashion and use this enhanced
knowledge to drastically (twelve times as much) augment the number of daily classified malicious domains.
\subsection{Summary} % (fold)
\label{sub:inthewild_summary}
Firstly \thesystem was bootstrapped using the \textsc{Exposure}~\cite{bilge2011exposure}
blacklist, extracting eleven clusters of domains referring to the same DGA. Then
for one week \thesystem analyzed a stream of passive DNS data collected in the
wild by a ISC/SIE DNS monitor. During that week 187 malicious domains were
detected and 1,300 IP addresses were labeled as ``suspicious''. At the end of
the week the clustering routine produced new knowledge in the form of 47 new
clusters, which were added to the ground truth. One of them was automatically
merged by \thesystem and a check of the IP addresses on the Web confirmed that
both clusters belong to the \texttt{Palevo} botnet.
\begin{samepage}
Therefore \thesystem was able to
\begin{enumerate}
\item perform on-line detection of known threats;
\item automatically detect new threats;
\item increase its knowledge;
\item use the new knowledge to classify previously unknown threats.
\end{enumerate}
\end{samepage}
We think that these results are quite encouraging and so prove the effectiveness
of \thesystem. Obviously there is more that can be done and there are some
difficulties to overcome. These matters shall be addressed in the next chapter.
% subsection inthewild_summary (end)
